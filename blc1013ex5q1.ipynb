{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "599288aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84abab22",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f81c6c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"C:\\Users\\krish\\Downloads\\archive (7)\\weatherHistory.csv\"\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5ba38e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krish\\AppData\\Local\\Temp\\ipykernel_34272\\1136123329.py:5: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  df[\"Formatted Date\"] = pd.to_datetime(df[\"Formatted Date\"])\n",
      "C:\\Users\\krish\\AppData\\Local\\Temp\\ipykernel_34272\\1136123329.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"Precip Type\"].fillna(\"unknown\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Drop unnecessary columns\n",
    "df = df.drop(columns=[\"Loud Cover\"])\n",
    "\n",
    "# Convert date column\n",
    "df[\"Formatted Date\"] = pd.to_datetime(df[\"Formatted Date\"])\n",
    "df = df.sort_values(\"Formatted Date\")\n",
    "df = df.drop(columns=[\"Formatted Date\"])\n",
    "\n",
    "# Fill missing values\n",
    "df[\"Precip Type\"].fillna(\"unknown\", inplace=True)\n",
    "df = pd.get_dummies(df, columns=[\"Precip Type\"], drop_first=True)\n",
    "\n",
    "# Encode textual features\n",
    "label_encoder_summary = LabelEncoder()\n",
    "df[\"Summary\"] = label_encoder_summary.fit_transform(df[\"Summary\"])\n",
    "\n",
    "label_encoder_daily_summary = LabelEncoder()\n",
    "df[\"Daily Summary\"] = label_encoder_daily_summary.fit_transform(df[\"Daily Summary\"])\n",
    "\n",
    "# Normalize data\n",
    "scaler = MinMaxScaler()\n",
    "data = scaler.fit_transform(df.values)\n",
    "\n",
    "# Create sequences\n",
    "def create_sequences(data, seq_length):\n",
    "    sequences, targets = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        sequences.append(data[i:i+seq_length])\n",
    "        targets.append(data[i+seq_length])\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "seq_length = 24  # Using past 24 hours to predict next timestep\n",
    "X, y = create_sequences(data, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "975ac252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_train, y_train = torch.tensor(X, dtype=torch.float32).to(device), torch.tensor(y, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e463239c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset class\n",
    "class WeatherDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X, self.y = X, y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Create dataloader\n",
    "dataset = WeatherDataset(X_train, y_train)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73a94191",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        return self.fc(out[:, -1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "600eea5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.fc(out[:, -1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1eb7fc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(BiLSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.fc(out[:, -1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3568c403",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=2):\n",
    "        super(StackedLSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.fc(out[:, -1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47db047b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_model(model, dataloader, epochs=10, lr=0.001):\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    for epoch in range(epochs):\n",
    "        for X_batch, y_batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e89998f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RNN...\n",
      "Epoch 1, Loss: 0.0086\n",
      "Epoch 2, Loss: 0.0081\n",
      "Epoch 3, Loss: 0.0103\n",
      "Epoch 4, Loss: 0.0098\n",
      "Epoch 5, Loss: 0.0111\n",
      "Epoch 6, Loss: 0.0190\n",
      "Epoch 7, Loss: 0.0086\n",
      "Epoch 8, Loss: 0.0120\n",
      "Epoch 9, Loss: 0.0120\n",
      "Epoch 10, Loss: 0.0099\n",
      "Training LSTM...\n",
      "Epoch 1, Loss: 0.0103\n",
      "Epoch 2, Loss: 0.0107\n",
      "Epoch 3, Loss: 0.0092\n",
      "Epoch 4, Loss: 0.0079\n",
      "Epoch 5, Loss: 0.0076\n",
      "Epoch 6, Loss: 0.0071\n",
      "Epoch 7, Loss: 0.0131\n",
      "Epoch 8, Loss: 0.0059\n",
      "Epoch 9, Loss: 0.0063\n",
      "Epoch 10, Loss: 0.0125\n",
      "Training BiLSTM...\n",
      "Epoch 1, Loss: 0.0088\n",
      "Epoch 2, Loss: 0.0129\n",
      "Epoch 3, Loss: 0.0145\n",
      "Epoch 4, Loss: 0.0107\n",
      "Epoch 5, Loss: 0.0112\n",
      "Epoch 6, Loss: 0.0076\n",
      "Epoch 7, Loss: 0.0102\n",
      "Epoch 8, Loss: 0.0091\n",
      "Epoch 9, Loss: 0.0107\n",
      "Epoch 10, Loss: 0.0126\n",
      "Training StackedLSTM...\n",
      "Epoch 1, Loss: 0.0113\n",
      "Epoch 2, Loss: 0.0071\n",
      "Epoch 3, Loss: 0.0120\n",
      "Epoch 4, Loss: 0.0080\n",
      "Epoch 5, Loss: 0.0096\n",
      "Epoch 6, Loss: 0.0109\n",
      "Epoch 7, Loss: 0.0066\n",
      "Epoch 8, Loss: 0.0093\n",
      "Epoch 9, Loss: 0.0079\n",
      "Epoch 10, Loss: 0.0085\n"
     ]
    }
   ],
   "source": [
    "# Train all models\n",
    "input_size = X_train.shape[2]\n",
    "hidden_size = 64\n",
    "output_size = y_train.shape[1]\n",
    "\n",
    "models = {\n",
    "    \"RNN\": RNNModel(input_size, hidden_size, output_size),\n",
    "    \"LSTM\": LSTMModel(input_size, hidden_size, output_size),\n",
    "    \"BiLSTM\": BiLSTMModel(input_size, hidden_size, output_size),\n",
    "    \"StackedLSTM\": StackedLSTMModel(input_size, hidden_size, output_size)\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    models[name] = train_model(model, dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91241bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN MSE: 0.0104\n",
      "LSTM MSE: 0.0097\n",
      "BiLSTM MSE: 0.0098\n",
      "StackedLSTM MSE: 0.0096\n"
     ]
    }
   ],
   "source": [
    "# Evaluate models with batch processing to avoid OOM error\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    criterion = nn.MSELoss()\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in dataloader:\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Evaluate each model in batches\n",
    "for name, model in models.items():\n",
    "    mse = evaluate_model(model, dataloader)\n",
    "    print(f\"{name} MSE: {mse:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (PyTorch)",
   "language": "python",
   "name": "python311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
