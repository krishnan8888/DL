{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da66bad2-3b36-4f6b-9e90-93814ca5b4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdb70f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1c3fba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1288a177",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = r\"C:\\Users\\krish\\Downloads\\oxford flowers 102.v1i.folder\"  #https://universe.roboflow.com/ck6jnertibvrhdvxefrfsu2pvcz2/oxford-flowers-102/dataset/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69a68847",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Keep at 224x224\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])'''\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Flip images randomly\n",
    "    transforms.RandomRotation(30),           # Rotate images\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),  # Adjust colors\n",
    "    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1)),  # Small shifts\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3d89b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = datasets.ImageFolder(root=f\"{DATASET_PATH}/train\", transform=transform)\n",
    "val_set = datasets.ImageFolder(root=f\"{DATASET_PATH}/valid\", transform=transform)\n",
    "test_set = datasets.ImageFolder(root=f\"{DATASET_PATH}/test\", transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdec37a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32  \n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ff80f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_1x1, red_3x3, out_3x3, red_5x5, out_5x5, pool_proj):\n",
    "        super(InceptionBlock, self).__init__()\n",
    "        # Branch 1: 1x1 convolution\n",
    "        self.branch1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_1x1, kernel_size=1),\n",
    "            nn.BatchNorm2d(out_1x1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # Branch 2: 1x1 convolution followed by 3x3 convolution\n",
    "        self.branch2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, red_3x3, kernel_size=1),\n",
    "            nn.BatchNorm2d(red_3x3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(red_3x3, out_3x3, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_3x3),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # Branch 3: 1x1 convolution followed by 5x5 convolution\n",
    "        self.branch3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, red_5x5, kernel_size=1),\n",
    "            nn.BatchNorm2d(red_5x5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(red_5x5, out_5x5, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(out_5x5),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # Branch 4: 3x3 max pooling followed by 1x1 convolution\n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv2d(in_channels, pool_proj, kernel_size=1),\n",
    "            nn.BatchNorm2d(pool_proj),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1 = self.branch1(x)\n",
    "        branch2 = self.branch2(x)\n",
    "        branch3 = self.branch3(x)\n",
    "        branch4 = self.branch4(x)\n",
    "        # Concatenate the outputs along the channel dimension\n",
    "        return torch.cat([branch1, branch2, branch3, branch4], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40ebeb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniGoogLeNet(nn.Module):\n",
    "    def __init__(self, num_classes=102):\n",
    "        super(MiniGoogLeNet, self).__init__()\n",
    "        # Initial convolution and pooling (similar to GoogLeNet's stem)\n",
    "        self.pre_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),  # 3 -> 64 channels\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        # Inception 3a: input channels = 64\n",
    "        self.inception3a = InceptionBlock(in_channels=64,\n",
    "                                          out_1x1=32,\n",
    "                                          red_3x3=48, out_3x3=64,\n",
    "                                          red_5x5=8,  out_5x5=16,\n",
    "                                          pool_proj=16)\n",
    "        # Output channels from inception3a: 32 + 64 + 16 + 16 = 128\n",
    "\n",
    "        # Inception 3b: input channels = 128\n",
    "        self.inception3b = InceptionBlock(in_channels=128,\n",
    "                                          out_1x1=64,\n",
    "                                          red_3x3=64, out_3x3=96,\n",
    "                                          red_5x5=16, out_5x5=48,\n",
    "                                          pool_proj=32)\n",
    "        # Output channels from inception3b: 64 + 96 + 48 + 32 = 240\n",
    "\n",
    "        # A max pooling layer to reduce spatial dimensions\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # Inception 4a: input channels = 240\n",
    "        self.inception4a = InceptionBlock(in_channels=240,\n",
    "                                          out_1x1=128,\n",
    "                                          red_3x3=128, out_3x3=192,\n",
    "                                          red_5x5=32,  out_5x5=96,\n",
    "                                          pool_proj=64)\n",
    "        # Output channels from inception4a: 128 + 192 + 96 + 64 = 480\n",
    "\n",
    "        # Global average pooling, dropout and final classifier\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.fc = nn.Linear(480, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pre_layers(x)\n",
    "        x = self.inception3a(x)\n",
    "        x = self.inception3b(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.inception4a(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate and move the model to device\n",
    "model = MiniGoogLeNet(num_classes=102).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60be8ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MiniGoogLeNet(num_classes=102).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ecf1e89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a21bc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "best_val_acc = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6dc7219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50] - Train Loss: 3.9386, Train Acc: 11.73% - Val Loss: 3.5057, Val Acc: 15.26% - Time: 34.69s\n",
      "Epoch [2/50] - Train Loss: 3.2524, Train Acc: 21.48% - Val Loss: 3.1131, Val Acc: 24.40% - Time: 34.18s\n",
      "Epoch [3/50] - Train Loss: 2.9400, Train Acc: 26.74% - Val Loss: 2.8300, Val Acc: 28.75% - Time: 33.78s\n",
      "Epoch [4/50] - Train Loss: 2.7125, Train Acc: 31.37% - Val Loss: 2.8161, Val Acc: 28.90% - Time: 33.81s\n",
      "Epoch [5/50] - Train Loss: 2.5868, Train Acc: 34.54% - Val Loss: 2.5989, Val Acc: 31.54% - Time: 33.78s\n",
      "Epoch [6/50] - Train Loss: 2.4509, Train Acc: 37.23% - Val Loss: 2.4213, Val Acc: 36.33% - Time: 34.00s\n",
      "Epoch [7/50] - Train Loss: 2.3000, Train Acc: 41.39% - Val Loss: 2.3301, Val Acc: 40.00% - Time: 34.23s\n",
      "Epoch [8/50] - Train Loss: 2.2055, Train Acc: 41.93% - Val Loss: 2.2304, Val Acc: 42.44% - Time: 34.13s\n",
      "Epoch [9/50] - Train Loss: 2.1150, Train Acc: 44.86% - Val Loss: 2.2105, Val Acc: 43.91% - Time: 34.19s\n",
      "Epoch [10/50] - Train Loss: 2.0363, Train Acc: 45.95% - Val Loss: 2.1886, Val Acc: 44.99% - Time: 33.82s\n",
      "Epoch [11/50] - Train Loss: 2.0016, Train Acc: 46.34% - Val Loss: 2.0902, Val Acc: 45.82% - Time: 34.17s\n",
      "Epoch [12/50] - Train Loss: 1.9205, Train Acc: 48.05% - Val Loss: 1.9902, Val Acc: 48.70% - Time: 33.99s\n",
      "Epoch [13/50] - Train Loss: 1.8242, Train Acc: 51.58% - Val Loss: 1.9177, Val Acc: 50.02% - Time: 34.19s\n",
      "Epoch [14/50] - Train Loss: 1.8143, Train Acc: 51.37% - Val Loss: 1.8792, Val Acc: 50.22% - Time: 34.21s\n",
      "Epoch [15/50] - Train Loss: 1.7286, Train Acc: 52.63% - Val Loss: 1.8321, Val Acc: 53.74% - Time: 33.81s\n",
      "Epoch [16/50] - Train Loss: 1.6980, Train Acc: 53.95% - Val Loss: 1.9776, Val Acc: 47.87% - Time: 33.71s\n",
      "Epoch [17/50] - Train Loss: 1.6087, Train Acc: 55.58% - Val Loss: 1.7493, Val Acc: 54.13% - Time: 33.84s\n",
      "Epoch [18/50] - Train Loss: 1.5745, Train Acc: 56.41% - Val Loss: 1.7050, Val Acc: 54.62% - Time: 33.78s\n",
      "Epoch [19/50] - Train Loss: 1.5272, Train Acc: 58.34% - Val Loss: 2.3330, Val Acc: 46.11% - Time: 34.16s\n",
      "Epoch [20/50] - Train Loss: 1.4839, Train Acc: 59.43% - Val Loss: 1.6965, Val Acc: 55.94% - Time: 33.80s\n",
      "Epoch [21/50] - Train Loss: 1.4267, Train Acc: 60.26% - Val Loss: 1.6223, Val Acc: 57.36% - Time: 33.98s\n",
      "Epoch [22/50] - Train Loss: 1.3912, Train Acc: 61.14% - Val Loss: 1.6116, Val Acc: 56.82% - Time: 34.00s\n",
      "Epoch [23/50] - Train Loss: 1.3874, Train Acc: 61.38% - Val Loss: 1.8070, Val Acc: 53.15% - Time: 34.17s\n",
      "Epoch [24/50] - Train Loss: 1.3494, Train Acc: 61.80% - Val Loss: 1.5354, Val Acc: 59.66% - Time: 34.02s\n",
      "Epoch [25/50] - Train Loss: 1.3586, Train Acc: 62.51% - Val Loss: 1.5493, Val Acc: 60.29% - Time: 33.97s\n",
      "Epoch [26/50] - Train Loss: 1.2914, Train Acc: 63.21% - Val Loss: 1.6553, Val Acc: 57.46% - Time: 33.78s\n",
      "Epoch [27/50] - Train Loss: 1.2533, Train Acc: 64.85% - Val Loss: 1.4842, Val Acc: 61.86% - Time: 34.02s\n",
      "Epoch [28/50] - Train Loss: 1.2342, Train Acc: 65.21% - Val Loss: 1.7708, Val Acc: 55.55% - Time: 33.76s\n",
      "Epoch [29/50] - Train Loss: 1.2059, Train Acc: 65.99% - Val Loss: 1.3796, Val Acc: 64.25% - Time: 33.80s\n",
      "Epoch [30/50] - Train Loss: 1.1100, Train Acc: 68.60% - Val Loss: 1.6509, Val Acc: 58.68% - Time: 33.78s\n",
      "Epoch [31/50] - Train Loss: 1.1577, Train Acc: 67.80% - Val Loss: 1.5559, Val Acc: 61.08% - Time: 33.79s\n",
      "Epoch [32/50] - Train Loss: 1.1318, Train Acc: 68.65% - Val Loss: 1.3772, Val Acc: 65.62% - Time: 33.82s\n",
      "Epoch [33/50] - Train Loss: 1.1031, Train Acc: 68.72% - Val Loss: 1.3516, Val Acc: 64.69% - Time: 33.98s\n",
      "Epoch [34/50] - Train Loss: 1.0871, Train Acc: 69.21% - Val Loss: 1.4213, Val Acc: 62.59% - Time: 33.98s\n",
      "Epoch [35/50] - Train Loss: 1.0519, Train Acc: 70.04% - Val Loss: 1.5191, Val Acc: 61.42% - Time: 33.77s\n",
      "Epoch [36/50] - Train Loss: 1.0177, Train Acc: 71.16% - Val Loss: 1.3547, Val Acc: 65.23% - Time: 33.97s\n",
      "Epoch [37/50] - Train Loss: 1.0060, Train Acc: 71.38% - Val Loss: 1.4636, Val Acc: 62.35% - Time: 33.59s\n",
      "Epoch [38/50] - Train Loss: 0.9892, Train Acc: 71.72% - Val Loss: 1.4006, Val Acc: 64.99% - Time: 33.78s\n",
      "Epoch [39/50] - Train Loss: 0.9462, Train Acc: 73.16% - Val Loss: 1.2210, Val Acc: 69.24% - Time: 34.07s\n",
      "Epoch [40/50] - Train Loss: 0.9342, Train Acc: 72.50% - Val Loss: 1.3274, Val Acc: 64.99% - Time: 33.92s\n",
      "Epoch [41/50] - Train Loss: 0.9340, Train Acc: 72.99% - Val Loss: 1.5515, Val Acc: 60.98% - Time: 33.59s\n",
      "Epoch [42/50] - Train Loss: 0.9280, Train Acc: 72.96% - Val Loss: 1.4632, Val Acc: 63.57% - Time: 33.58s\n",
      "Epoch [43/50] - Train Loss: 0.8950, Train Acc: 74.18% - Val Loss: 1.3346, Val Acc: 67.43% - Time: 33.79s\n",
      "Epoch [44/50] - Train Loss: 0.8497, Train Acc: 75.52% - Val Loss: 1.2290, Val Acc: 67.19% - Time: 33.79s\n",
      "Epoch [45/50] - Train Loss: 0.8751, Train Acc: 74.67% - Val Loss: 1.3948, Val Acc: 65.23% - Time: 33.98s\n",
      "Epoch [46/50] - Train Loss: 0.8814, Train Acc: 74.38% - Val Loss: 1.3783, Val Acc: 65.53% - Time: 33.79s\n",
      "Epoch [47/50] - Train Loss: 0.8546, Train Acc: 75.79% - Val Loss: 1.3008, Val Acc: 67.53% - Time: 34.17s\n",
      "Epoch [48/50] - Train Loss: 0.8297, Train Acc: 75.38% - Val Loss: 1.2367, Val Acc: 69.19% - Time: 33.82s\n",
      "Epoch [49/50] - Train Loss: 0.8267, Train Acc: 75.43% - Val Loss: 1.1679, Val Acc: 70.37% - Time: 33.84s\n",
      "Epoch [50/50] - Train Loss: 0.8198, Train Acc: 76.60% - Val Loss: 1.3262, Val Acc: 67.09% - Time: 33.72s\n",
      "Training complete. Best Validation Accuracy: 70.36674816625917\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_acc = 100 * correct / total\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # **Validation**\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    val_acc = 100 * correct / total\n",
    "    val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    # **Save Best Model**\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% - \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}% - \"\n",
    "          f\"Time: {end_time - start_time:.2f}s\")\n",
    "\n",
    "print(\"Training complete. Best Validation Accuracy:\", best_val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9343d0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Testing**\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea693c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 69.59%\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "test_acc = 100 * correct / total\n",
    "print(f\"Test Accuracy: {test_acc:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (PyTorch)",
   "language": "python",
   "name": "python311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
