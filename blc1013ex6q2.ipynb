{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6256fe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from torch.utils.data import random_split\n",
    "import gc\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75fdac2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e3d7f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold):\n",
    "        \"\"\"\n",
    "        freq_threshold: minimum frequency for a word to be included.\n",
    "        \"\"\"\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "        # Special tokens.\n",
    "        self.add_word(\"<PAD>\")\n",
    "        self.add_word(\"<SOS>\")\n",
    "        self.add_word(\"<EOS>\")\n",
    "        self.add_word(\"<UNK>\")\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "    def tokenize(self, text):\n",
    "        return str(text).lower().strip().split()\n",
    "\n",
    "\n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        frequencies = {}\n",
    "        for sentence in sentence_list:\n",
    "            tokens = self.tokenize(sentence)\n",
    "            for token in tokens:\n",
    "                frequencies[token] = frequencies.get(token, 0) + 1\n",
    "\n",
    "        for word, freq in frequencies.items():\n",
    "            if freq >= self.freq_threshold:\n",
    "                self.add_word(word)\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = self.tokenize(text)\n",
    "        return [self.word2idx.get(token, self.word2idx[\"<UNK>\"]) for token in tokenized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c81b71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionGenerationDataset(Dataset):\n",
    "    def __init__(self, csv_file, image_folder, vocabulary, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file: Path to the CSV file.\n",
    "            image_folder: Folder containing images.\n",
    "            vocabulary: An instance of the Vocabulary class.\n",
    "            transform: Image transformations.\n",
    "            \n",
    "        The CSV is assumed to have:\n",
    "          - Column 0: a URL (the image filename is the last 16 characters: 12 chars + \".jpg\")\n",
    "          - Column 6: the question text.\n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.image_folder = image_folder\n",
    "        self.vocab = vocabulary\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        # Extract image name from the first column (last 16 characters)\n",
    "        link = row.iloc[0]\n",
    "        image_name = link[-16:]\n",
    "        image_path = os.path.join(self.image_folder, image_name)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Get question text from column 7 (index 6)\n",
    "        question = row.iloc[6]\n",
    "\n",
    "        # Numericalize the question and add <SOS> and <EOS> tokens.\n",
    "        tokens = [self.vocab.word2idx[\"<SOS>\"]]\n",
    "        tokens += self.vocab.numericalize(question)\n",
    "        tokens.append(self.vocab.word2idx[\"<EOS>\"])\n",
    "        caption = torch.tensor(tokens, dtype=torch.long)\n",
    "        return image, caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9f4dd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    \"\"\"\n",
    "    Sort a batch of data by caption length (descending order) and pad sequences.\n",
    "    \"\"\"\n",
    "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    images, captions = zip(*data)\n",
    "    images = torch.stack(images, 0)\n",
    "    lengths = [len(cap) for cap in captions]\n",
    "    padded_captions = rnn_utils.pad_sequence(captions, batch_first=True, padding_value=0)\n",
    "    return images, padded_captions, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de86ff54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        modules = list(resnet.children())[:-1]  # remove the final fully connected layer\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.fc = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
    "    \n",
    "    def forward(self, images):\n",
    "        # Extract features without computing gradients.\n",
    "        with torch.no_grad():\n",
    "            features = self.resnet(images)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        features = self.bn(self.fc(features))\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5294f9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, features, captions, lengths):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: Encoded image features of shape (batch, embed_size)\n",
    "            captions: Tensor of token indices (batch, seq_length). Note that during training, we feed\n",
    "                      the ground-truth captions (teacher forcing).\n",
    "            lengths: List of actual lengths for each caption.\n",
    "            \n",
    "        The features are prepended as the first input to the LSTM.\n",
    "        \"\"\"\n",
    "    \n",
    "        embeddings = self.embed(captions)  # (batch, L, embed_size)\n",
    "        # Initialize LSTM hidden state using image features.\n",
    "        h0 = features.unsqueeze(0).repeat(self.num_layers, 1, 1)  # (num_layers, batch, embed_size)\n",
    "        c0 = torch.zeros_like(h0)\n",
    "        # Use original lengths (input captions length remains the same) after shifting targets.\n",
    "        packed = rnn_utils.pack_padded_sequence(embeddings, lengths, batch_first=True, enforce_sorted=False)\n",
    "        hiddens, _ = self.lstm(packed, (h0, c0))\n",
    "        outputs = self.linear(hiddens[0])\n",
    "        return outputs\n",
    "    \n",
    "    def sample(self, features, vocab, max_len=20):\n",
    "        batch_size = features.size(0)\n",
    "        sampled_ids = []\n",
    "        # Create a batch of <SOS> tokens.\n",
    "        inputs = self.embed(torch.tensor([vocab.word2idx[\"<SOS>\"]]*batch_size).to(features.device)).unsqueeze(1)\n",
    "        # Initialize LSTM hidden state with image features.\n",
    "        h0 = features.unsqueeze(0).repeat(self.num_layers, 1, 1)  # Shape: (num_layers, batch_size, embed_size)\n",
    "        c0 = torch.zeros_like(h0)\n",
    "        states = (h0, c0)\n",
    "        for i in range(max_len):\n",
    "            hiddens, states = self.lstm(inputs, states)\n",
    "            outputs = self.linear(hiddens.squeeze(1))\n",
    "            predicted = outputs.argmax(1)\n",
    "            sampled_ids.append(predicted)\n",
    "            inputs = self.embed(predicted).unsqueeze(1)\n",
    "        sampled_ids = torch.stack(sampled_ids, 1)\n",
    "        return sampled_ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d151bebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(encoder, decoder, dataloader, vocab, device, max_len=20):\n",
    "    \"\"\"\n",
    "    Runs the model on the validation set and computes the average BLEU score.\n",
    "    \"\"\"\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    total_bleu = 0.0\n",
    "    total_examples = 0\n",
    "    with torch.no_grad():\n",
    "        for images, captions, lengths in dataloader:\n",
    "            images = images.to(device)\n",
    "            features = encoder(images)\n",
    "            #sampled_ids = decoder.sample(features, max_len=max_len)\n",
    "            sampled_ids = decoder.sample(features, vocab, max_len=max_len)\n",
    "            for i in range(images.size(0)):\n",
    "                sampled_seq = sampled_ids[i].cpu().numpy()\n",
    "                # Convert predicted ids to words; stop at <EOS>\n",
    "                pred_words = []\n",
    "                for word_id in sampled_seq:\n",
    "                    word = vocab.idx2word[word_id]\n",
    "                    if word == \"<EOS>\":\n",
    "                        break\n",
    "                    pred_words.append(word)\n",
    "                # Process the ground-truth caption: remove <SOS> and stop at <EOS>\n",
    "                true_caption = captions[i].cpu().numpy()\n",
    "                true_words = []\n",
    "                for word_id in true_caption:\n",
    "                    word = vocab.idx2word.get(int(word_id), \"<UNK>\")\n",
    "                    if word == \"<SOS>\":\n",
    "                        continue\n",
    "                    if word == \"<EOS>\":\n",
    "                        break\n",
    "                    true_words.append(word)\n",
    "                # Compute sentence-level BLEU score.\n",
    "                bleu = sentence_bleu([true_words], pred_words, \n",
    "                                     smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method1)\n",
    "                total_bleu += bleu\n",
    "                total_examples += 1\n",
    "    avg_bleu = total_bleu / total_examples if total_examples > 0 else 0\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    return avg_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb5d14db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Hyperparameters.\n",
    "    embed_size = 256\n",
    "    hidden_size = 256\n",
    "    num_layers = 1\n",
    "    num_epochs = 5\n",
    "    batch_size = 32\n",
    "    learning_rate = 1e-3\n",
    "    freq_threshold = 5  # Only include words that occur at least 5 times.\n",
    "\n",
    "    # Image transformations.\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Build vocabulary from the CSV.\n",
    "    csv_file = r\"C:\\Users\\krish\\Downloads\\train.csv\"\n",
    "    df = pd.read_csv(csv_file)\n",
    "    questions = df.iloc[:, 6].tolist()  # Column 7 (index 6) has question text.\n",
    "    vocab = Vocabulary(freq_threshold)\n",
    "    vocab.build_vocabulary(questions)\n",
    "    print(\"Vocabulary size:\", len(vocab.word2idx))\n",
    "\n",
    "    # Create the full dataset.\n",
    "    image_folder = r\"C:\\Users\\krish\\Downloads\\train\"\n",
    "    full_dataset = QuestionGenerationDataset(csv_file, image_folder, vocab, transform)\n",
    "\n",
    "    # Split dataset into training (90%) and validation (10%) sets.\n",
    "    train_size = int(0.9 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # Device configuration.\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Initialize encoder and decoder.\n",
    "    encoder = ImageEncoder(embed_size).to(device)\n",
    "    decoder = DecoderRNN(embed_size, hidden_size, len(vocab.word2idx), num_layers).to(device)\n",
    "\n",
    "    # Only update decoder parameters and the encoder's fc and bn layers.\n",
    "    params = list(decoder.parameters()) + list(encoder.fc.parameters()) + list(encoder.bn.parameters())\n",
    "    optimizer = optim.Adam(params, lr=learning_rate)\n",
    "    # Use CrossEntropyLoss and ignore the padding index.\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=vocab.word2idx[\"<PAD>\"])\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        epoch_loss = 0.0\n",
    "        total_samples = 0\n",
    "\n",
    "        for images, captions, lengths in train_loader:\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "\n",
    "            # === Teacher Forcing Changes (Third Change) ===\n",
    "            # Input is captions excluding the last token; target is captions excluding the first token.\n",
    "            inputs = captions[:, :-1]\n",
    "            targets = captions[:, 1:]\n",
    "            # Adjust lengths: each length is reduced by 1.\n",
    "            input_lengths = [l - 1 for l in lengths]\n",
    "            # Pack targets with adjusted lengths.\n",
    "            targets_packed = rnn_utils.pack_padded_sequence(targets, input_lengths, batch_first=True, enforce_sorted=False)[0]\n",
    "\n",
    "            # Forward pass.\n",
    "            features = encoder(images)\n",
    "            outputs = decoder(features, inputs, input_lengths)\n",
    "            loss = criterion(outputs, targets_packed)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item() * images.size(0)\n",
    "            total_samples += images.size(0)\n",
    "\n",
    "        avg_train_loss = epoch_loss / total_samples\n",
    "        avg_bleu = validate(encoder, decoder, val_loader, vocab, device, max_len=20)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Avg Train Loss: {avg_train_loss:.4f}, Avg BLEU: {avg_bleu:.4f}\")\n",
    "\n",
    "    # Save the model and vocabulary.\n",
    "    torch.save({'encoder_state_dict': encoder.state_dict(),\n",
    "                'decoder_state_dict': decoder.state_dict(),\n",
    "                'vocab': vocab}, 'question_generation.pth')\n",
    "\n",
    "    # Generate a question for a sample image.\n",
    "    sample_image_path = os.path.join(image_folder, df.iloc[0, 0][-16:])\n",
    "    question = generate_question(sample_image_path, encoder, decoder, vocab, transform, device)\n",
    "    print(\"Generated question:\", question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5edc21f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_question(image_path, encoder, decoder, vocab, transform, device, max_len=20):\n",
    "    \"\"\"\n",
    "    Given an image file, generate a question using the trained model.\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension.\n",
    "    image = image.to(device)\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        feature = encoder(image)\n",
    "        sampled_ids = decoder.sample(feature, max_len=max_len)\n",
    "    sampled_ids = sampled_ids[0].cpu().numpy()\n",
    "    # Convert word IDs back to words.\n",
    "    words = []\n",
    "    for word_id in sampled_ids:\n",
    "        word = vocab.idx2word[word_id]\n",
    "        if word == \"<EOS>\":\n",
    "            break\n",
    "        words.append(word)\n",
    "    question = ' '.join(words)\n",
    "    return question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea88d078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] - Avg Train Loss: 3.1641, Avg BLEU: 0.0014\n",
      "Epoch [2/5] - Avg Train Loss: 3.0878, Avg BLEU: 0.0015\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 57\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     54\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     55\u001b[0m total_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 57\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcaptions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcaptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[1;32mIn[4], line 28\u001b[0m, in \u001b[0;36mQuestionGenerationDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     26\u001b[0m image_name \u001b[38;5;241m=\u001b[39m link[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m16\u001b[39m:]\n\u001b[0;32m     27\u001b[0m image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_folder, image_name)\n\u001b[1;32m---> 28\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[0;32m     30\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\Image.py:993\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    990\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;15\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;16\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;24\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    991\u001b[0m     deprecate(mode, \u001b[38;5;241m12\u001b[39m)\n\u001b[1;32m--> 993\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    995\u001b[0m has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[0;32m    996\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    997\u001b[0m     \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\ImageFile.py:300\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[0;32m    299\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 300\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc1e190",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abf806f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (PyTorch)",
   "language": "python",
   "name": "python311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
