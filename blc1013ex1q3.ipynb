{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f79cba9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Activation functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "def leaky_relu_derivative(x, alpha=0.01):\n",
    "    return np.where(x > 0, 1, alpha)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x)**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2a06c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleLayerPerceptron:\n",
    "    def __init__(self, activation=\"sigmoid\"):\n",
    "        self.input_size = 2\n",
    "        self.output_size = 2\n",
    "        self.weights = np.random.rand(self.input_size, self.output_size) - 0.5\n",
    "        self.bias = np.random.rand(self.output_size) - 0.5\n",
    "\n",
    "        # Set activation functions\n",
    "        activations = {\n",
    "            \"sigmoid\": (sigmoid, sigmoid_derivative),\n",
    "            \"relu\": (relu, relu_derivative),\n",
    "            \"leaky_relu\": (leaky_relu, leaky_relu_derivative),\n",
    "            \"tanh\": (tanh, tanh_derivative),\n",
    "        }\n",
    "        if activation not in activations:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "        self.activation, self.activation_derivative = activations[activation]\n",
    "\n",
    "    def feedforward(self, X):\n",
    "        self.input = np.dot(X, self.weights) + self.bias\n",
    "        self.output = self.activation(self.input)\n",
    "        return self.output\n",
    "\n",
    "    def train(self, X, y, epochs=1000, learning_rate=0.1):\n",
    "        for epoch in range(epochs):\n",
    "            self.feedforward(X)\n",
    "            error = self.output - y\n",
    "            output_delta = error * self.activation_derivative(self.input)\n",
    "            self.weights -= learning_rate * np.dot(X.T, output_delta)\n",
    "            self.bias -= learning_rate * np.sum(output_delta, axis=0)\n",
    "            if epoch % 100 == 0:\n",
    "                loss = np.mean(np.abs(error))\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.feedforward(X), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ddd6381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Single-Layer Perceptron (Sigmoid):\n",
      "Epoch 0, Loss: 0.5014\n",
      "Epoch 100, Loss: 0.5000\n",
      "Epoch 200, Loss: 0.5000\n",
      "Epoch 300, Loss: 0.5000\n",
      "Epoch 400, Loss: 0.5000\n",
      "Epoch 500, Loss: 0.5000\n",
      "Epoch 600, Loss: 0.5000\n",
      "Epoch 700, Loss: 0.5000\n",
      "Epoch 800, Loss: 0.5000\n",
      "Epoch 900, Loss: 0.5000\n",
      "Epoch 1000, Loss: 0.5000\n",
      "Epoch 1100, Loss: 0.5000\n",
      "Epoch 1200, Loss: 0.5000\n",
      "Epoch 1300, Loss: 0.5000\n",
      "Epoch 1400, Loss: 0.5000\n",
      "Epoch 1500, Loss: 0.5000\n",
      "Epoch 1600, Loss: 0.5000\n",
      "Epoch 1700, Loss: 0.5000\n",
      "Epoch 1800, Loss: 0.5000\n",
      "Epoch 1900, Loss: 0.5000\n",
      "Epoch 2000, Loss: 0.5000\n",
      "Epoch 2100, Loss: 0.5000\n",
      "Epoch 2200, Loss: 0.5000\n",
      "Epoch 2300, Loss: 0.5000\n",
      "Epoch 2400, Loss: 0.5000\n",
      "Epoch 2500, Loss: 0.5000\n",
      "Epoch 2600, Loss: 0.5000\n",
      "Epoch 2700, Loss: 0.5000\n",
      "Epoch 2800, Loss: 0.5000\n",
      "Epoch 2900, Loss: 0.5000\n",
      "Epoch 3000, Loss: 0.5000\n",
      "Epoch 3100, Loss: 0.5000\n",
      "Epoch 3200, Loss: 0.5000\n",
      "Epoch 3300, Loss: 0.5000\n",
      "Epoch 3400, Loss: 0.5000\n",
      "Epoch 3500, Loss: 0.5000\n",
      "Epoch 3600, Loss: 0.5000\n",
      "Epoch 3700, Loss: 0.5000\n",
      "Epoch 3800, Loss: 0.5000\n",
      "Epoch 3900, Loss: 0.5000\n",
      "Epoch 4000, Loss: 0.5000\n",
      "Epoch 4100, Loss: 0.5000\n",
      "Epoch 4200, Loss: 0.5000\n",
      "Epoch 4300, Loss: 0.5000\n",
      "Epoch 4400, Loss: 0.5000\n",
      "Epoch 4500, Loss: 0.5000\n",
      "Epoch 4600, Loss: 0.5000\n",
      "Epoch 4700, Loss: 0.5000\n",
      "Epoch 4800, Loss: 0.5000\n",
      "Epoch 4900, Loss: 0.5000\n",
      "\n",
      "Predictions:\n",
      "Input: [0 0], Prediction: [1], Target: 0\n",
      "Input: [0 1], Prediction: [1], Target: 1\n",
      "Input: [1 0], Prediction: [1], Target: 1\n",
      "Input: [1 1], Prediction: [0], Target: 0\n"
     ]
    }
   ],
   "source": [
    "# XOR Problem\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[1, 0], [0, 1], [0, 1], [1, 0]])  # One-hot encoding\n",
    "\n",
    "# Example: Training with Sigmoid activation\n",
    "slp = SingleLayerPerceptron(activation=\"sigmoid\")\n",
    "print(\"\\nTraining Single-Layer Perceptron (Sigmoid):\")\n",
    "slp.train(X, y, epochs=5000, learning_rate=0.1)\n",
    "\n",
    "# Example: Predictions\n",
    "print(\"\\nPredictions:\")\n",
    "for sample, target in zip(X, y):\n",
    "    prediction = slp.predict(sample.reshape(1, -1))\n",
    "    print(f\"Input: {sample}, Prediction: {prediction}, Target: {np.argmax(target)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b683fdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Layer Perceptron class\n",
    "class MultiLayerPerceptron:\n",
    "    def __init__(self, hidden_size=4, activation=\"sigmoid\"):\n",
    "        self.input_size = 2\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = 2\n",
    "        self.weights_input_hidden = np.random.rand(self.input_size, self.hidden_size) - 0.5\n",
    "        self.weights_hidden_output = np.random.rand(self.hidden_size, self.output_size) - 0.5\n",
    "        self.bias_hidden = np.random.rand(self.hidden_size) - 0.5\n",
    "        self.bias_output = np.random.rand(self.output_size) - 0.5\n",
    "\n",
    "        # Set activation functions\n",
    "        activations = {\n",
    "            \"sigmoid\": (sigmoid, sigmoid_derivative),\n",
    "            \"relu\": (relu, relu_derivative),\n",
    "            \"leaky_relu\": (leaky_relu, leaky_relu_derivative),\n",
    "            \"tanh\": (tanh, tanh_derivative),\n",
    "        }\n",
    "        if activation not in activations:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "        self.activation, self.activation_derivative = activations[activation]\n",
    "\n",
    "    def feedforward(self, X):\n",
    "        # Hidden layer\n",
    "        self.hidden_input = np.dot(X, self.weights_input_hidden) + self.bias_hidden\n",
    "        self.hidden_output = self.activation(self.hidden_input)\n",
    "\n",
    "        # Output layer\n",
    "        self.output_input = np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output\n",
    "        exp_output = np.exp(self.output_input - np.max(self.output_input, axis=1, keepdims=True))  # Avoid overflow\n",
    "        self.output = exp_output / np.sum(exp_output, axis=1, keepdims=True)\n",
    "        return self.output\n",
    "\n",
    "    def backpropagate(self, X, y, learning_rate=0.1):\n",
    "        output_error = self.output - y\n",
    "        output_delta = output_error  # Softmax + cross-entropy\n",
    "\n",
    "        hidden_error = np.dot(output_delta, self.weights_hidden_output.T)\n",
    "        hidden_delta = hidden_error * self.activation_derivative(self.hidden_input)\n",
    "\n",
    "        self.weights_hidden_output -= learning_rate * np.dot(self.hidden_output.T, output_delta)\n",
    "        self.bias_output -= learning_rate * np.sum(output_delta, axis=0)\n",
    "\n",
    "        self.weights_input_hidden -= learning_rate * np.dot(X.T, hidden_delta)\n",
    "        self.bias_hidden -= learning_rate * np.sum(hidden_delta, axis=0)\n",
    "\n",
    "    def train(self, X, y, epochs=1000, learning_rate=0.1):\n",
    "        for epoch in range(epochs):\n",
    "            self.feedforward(X)\n",
    "            self.backpropagate(X, y, learning_rate)\n",
    "            if epoch % 100 == 0:\n",
    "                loss = -np.sum(y * np.log(self.output + 1e-9)) / X.shape[0]\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        output = self.feedforward(X)\n",
    "        return np.argmax(output, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f97aef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Multi-Layer Perceptron (Sigmoid):\n",
      "Epoch 0, Loss: 0.7785\n",
      "Epoch 100, Loss: 0.6935\n",
      "Epoch 200, Loss: 0.6933\n",
      "Epoch 300, Loss: 0.6932\n",
      "Epoch 400, Loss: 0.6932\n",
      "Epoch 500, Loss: 0.6931\n",
      "Epoch 600, Loss: 0.6931\n",
      "Epoch 700, Loss: 0.6931\n",
      "Epoch 800, Loss: 0.6930\n",
      "Epoch 900, Loss: 0.6929\n",
      "Epoch 1000, Loss: 0.6928\n",
      "Epoch 1100, Loss: 0.6925\n",
      "Epoch 1200, Loss: 0.6918\n",
      "Epoch 1300, Loss: 0.6898\n",
      "Epoch 1400, Loss: 0.6836\n",
      "Epoch 1500, Loss: 0.6641\n",
      "Epoch 1600, Loss: 0.6133\n",
      "Epoch 1700, Loss: 0.5227\n",
      "Epoch 1800, Loss: 0.3954\n",
      "Epoch 1900, Loss: 0.2370\n",
      "Epoch 2000, Loss: 0.1288\n",
      "Epoch 2100, Loss: 0.0777\n",
      "Epoch 2200, Loss: 0.0527\n",
      "Epoch 2300, Loss: 0.0389\n",
      "Epoch 2400, Loss: 0.0303\n",
      "Epoch 2500, Loss: 0.0247\n",
      "Epoch 2600, Loss: 0.0207\n",
      "Epoch 2700, Loss: 0.0177\n",
      "Epoch 2800, Loss: 0.0154\n",
      "Epoch 2900, Loss: 0.0137\n",
      "Epoch 3000, Loss: 0.0122\n",
      "Epoch 3100, Loss: 0.0111\n",
      "Epoch 3200, Loss: 0.0101\n",
      "Epoch 3300, Loss: 0.0093\n",
      "Epoch 3400, Loss: 0.0086\n",
      "Epoch 3500, Loss: 0.0079\n",
      "Epoch 3600, Loss: 0.0074\n",
      "Epoch 3700, Loss: 0.0069\n",
      "Epoch 3800, Loss: 0.0065\n",
      "Epoch 3900, Loss: 0.0062\n",
      "Epoch 4000, Loss: 0.0058\n",
      "Epoch 4100, Loss: 0.0055\n",
      "Epoch 4200, Loss: 0.0052\n",
      "Epoch 4300, Loss: 0.0050\n",
      "Epoch 4400, Loss: 0.0048\n",
      "Epoch 4500, Loss: 0.0046\n",
      "Epoch 4600, Loss: 0.0044\n",
      "Epoch 4700, Loss: 0.0042\n",
      "Epoch 4800, Loss: 0.0040\n",
      "Epoch 4900, Loss: 0.0039\n",
      "\n",
      "Predictions:\n",
      "Input: [0 0], Prediction: [0], Target: 0\n",
      "Input: [0 1], Prediction: [1], Target: 1\n",
      "Input: [1 0], Prediction: [1], Target: 1\n",
      "Input: [1 1], Prediction: [0], Target: 0\n"
     ]
    }
   ],
   "source": [
    "# Example: Training with Sigmoid activation for MLP\n",
    "mlp = MultiLayerPerceptron(hidden_size=4, activation=\"sigmoid\")\n",
    "print(\"\\nTraining Multi-Layer Perceptron (Sigmoid):\")\n",
    "mlp.train(X, y, epochs=5000, learning_rate=0.1)\n",
    "\n",
    "# Example: Predictions\n",
    "print(\"\\nPredictions:\")\n",
    "for sample, target in zip(X, y):\n",
    "    prediction = mlp.predict(sample.reshape(1, -1))\n",
    "    print(f\"Input: {sample}, Prediction: {prediction}, Target: {np.argmax(target)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41abcc02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a793e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb012a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
