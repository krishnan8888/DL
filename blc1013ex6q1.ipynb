{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de0e8e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f93add0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = r\"C:\\Users\\krish\\Downloads\\archive (9)\\hin.txt\"  \n",
    "\n",
    "english_sentences = []\n",
    "hindi_sentences = []\n",
    "\n",
    "with open(data_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(\"\\t\")  # Tab-separated values\n",
    "        if len(parts) >= 2:\n",
    "            english_sentences.append(parts[0])  # English text\n",
    "            hindi_sentences.append(parts[1])  # Hindi text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbc96a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_tokenizer = Tokenizer(filters='', oov_token=\"<OOV>\")\n",
    "hin_tokenizer = Tokenizer(filters='', oov_token=\"<OOV>\")\n",
    "\n",
    "eng_tokenizer.fit_on_texts(english_sentences)\n",
    "hin_tokenizer.fit_on_texts(hindi_sentences)\n",
    "\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "hin_vocab_size = len(hin_tokenizer.word_index) + 1\n",
    "\n",
    "eng_sequences = eng_tokenizer.texts_to_sequences(english_sentences)\n",
    "hin_sequences = hin_tokenizer.texts_to_sequences(hindi_sentences)\n",
    "\n",
    "max_eng_length = max(len(seq) for seq in eng_sequences)\n",
    "max_hin_length = max(len(seq) for seq in hin_sequences)\n",
    "\n",
    "eng_padded = pad_sequences(eng_sequences, maxlen=max_eng_length, padding=\"post\")\n",
    "hin_padded = pad_sequences(hin_sequences, maxlen=max_hin_length, padding=\"post\")\n",
    "\n",
    "hin_padded_cat = np.array([to_categorical(seq, num_classes=hin_vocab_size) for seq in hin_padded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7320efc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36564845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_encoder():\n",
    "    encoder_inputs = Input(shape=(max_eng_length,))\n",
    "    enc_emb = Embedding(eng_vocab_size, latent_dim, mask_zero=True)(encoder_inputs)\n",
    "    encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "    encoder_states = [state_h, state_c]\n",
    "    \n",
    "    return Model(encoder_inputs, encoder_states), encoder_inputs, encoder_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48587065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_decoder(encoder_states):\n",
    "    decoder_inputs = Input(shape=(max_hin_length,))\n",
    "    \n",
    "    # Embedding layer for decoder\n",
    "    dec_emb_layer = Embedding(hin_vocab_size, latent_dim, mask_zero=True)\n",
    "    dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "    # LSTM layer\n",
    "    decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
    "\n",
    "    # Dense output layer\n",
    "    decoder_dense = Dense(hin_vocab_size, activation=\"softmax\")\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    # Training model (outputs full sequence)\n",
    "    decoder_model_train = Model([decoder_inputs] + encoder_states, decoder_outputs)\n",
    "\n",
    "    # Inference model for decoding one step at a time\n",
    "    decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "    decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "    decoder_hidden_state_input = Input(shape=(max_eng_length, latent_dim))  # Encoder output\n",
    "\n",
    "    dec_emb_inf = dec_emb_layer(decoder_inputs)\n",
    "    decoder_outputs_inf, state_h_inf, state_c_inf = decoder_lstm(\n",
    "        dec_emb_inf, initial_state=[decoder_state_input_h, decoder_state_input_c]\n",
    "    )\n",
    "    decoder_outputs_inf = decoder_dense(decoder_outputs_inf)\n",
    "\n",
    "    decoder_model_inf = Model(\n",
    "        [decoder_inputs] + [decoder_state_input_h, decoder_state_input_c, decoder_hidden_state_input],\n",
    "        [decoder_outputs_inf, state_h_inf, state_c_inf]\n",
    "    )\n",
    "\n",
    "    return decoder_model_train, decoder_model_inf, decoder_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "237fb683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "40/40 [==============================] - 26s 398ms/step - loss: 7.2504 - accuracy: 0.0750 - val_loss: 6.8206 - val_accuracy: 0.0369\n",
      "Epoch 2/40\n",
      "40/40 [==============================] - 13s 330ms/step - loss: 6.1371 - accuracy: 0.0710 - val_loss: 6.4513 - val_accuracy: 0.0441\n",
      "Epoch 3/40\n",
      "40/40 [==============================] - 12s 290ms/step - loss: 5.6079 - accuracy: 0.0795 - val_loss: 6.1812 - val_accuracy: 0.0739\n",
      "Epoch 4/40\n",
      "40/40 [==============================] - 12s 302ms/step - loss: 5.2595 - accuracy: 0.1552 - val_loss: 5.9137 - val_accuracy: 0.1438\n",
      "Epoch 5/40\n",
      "40/40 [==============================] - 13s 320ms/step - loss: 4.8523 - accuracy: 0.2306 - val_loss: 5.5322 - val_accuracy: 0.2393\n",
      "Epoch 6/40\n",
      "40/40 [==============================] - 12s 310ms/step - loss: 4.3701 - accuracy: 0.3222 - val_loss: 5.0911 - val_accuracy: 0.3109\n",
      "Epoch 7/40\n",
      "40/40 [==============================] - 13s 321ms/step - loss: 3.8963 - accuracy: 0.4062 - val_loss: 4.6686 - val_accuracy: 0.3929\n",
      "Epoch 8/40\n",
      "40/40 [==============================] - 12s 292ms/step - loss: 3.4564 - accuracy: 0.4897 - val_loss: 4.3078 - val_accuracy: 0.4691\n",
      "Epoch 9/40\n",
      "40/40 [==============================] - 12s 288ms/step - loss: 3.0629 - accuracy: 0.5574 - val_loss: 4.0141 - val_accuracy: 0.5204\n",
      "Epoch 10/40\n",
      "40/40 [==============================] - 12s 289ms/step - loss: 2.7247 - accuracy: 0.6134 - val_loss: 3.7664 - val_accuracy: 0.5714\n",
      "Epoch 11/40\n",
      "40/40 [==============================] - 11s 286ms/step - loss: 2.4299 - accuracy: 0.6604 - val_loss: 3.5596 - val_accuracy: 0.6126\n",
      "Epoch 12/40\n",
      "40/40 [==============================] - 11s 288ms/step - loss: 2.1686 - accuracy: 0.7023 - val_loss: 3.3925 - val_accuracy: 0.6424\n",
      "Epoch 13/40\n",
      "40/40 [==============================] - 12s 288ms/step - loss: 1.9401 - accuracy: 0.7343 - val_loss: 3.2421 - val_accuracy: 0.6613\n",
      "Epoch 14/40\n",
      "40/40 [==============================] - 12s 290ms/step - loss: 1.7397 - accuracy: 0.7626 - val_loss: 3.1179 - val_accuracy: 0.6868\n",
      "Epoch 15/40\n",
      "40/40 [==============================] - 12s 304ms/step - loss: 1.5626 - accuracy: 0.7888 - val_loss: 3.0319 - val_accuracy: 0.7035\n",
      "Epoch 16/40\n",
      "40/40 [==============================] - 11s 274ms/step - loss: 1.4058 - accuracy: 0.8114 - val_loss: 2.9388 - val_accuracy: 0.7212\n",
      "Epoch 17/40\n",
      "40/40 [==============================] - 11s 276ms/step - loss: 1.2621 - accuracy: 0.8325 - val_loss: 2.8496 - val_accuracy: 0.7414\n",
      "Epoch 18/40\n",
      "40/40 [==============================] - 11s 275ms/step - loss: 1.1356 - accuracy: 0.8540 - val_loss: 2.7879 - val_accuracy: 0.7512\n",
      "Epoch 19/40\n",
      "40/40 [==============================] - 11s 267ms/step - loss: 1.0149 - accuracy: 0.8708 - val_loss: 2.7416 - val_accuracy: 0.7607\n",
      "Epoch 20/40\n",
      "40/40 [==============================] - 11s 276ms/step - loss: 0.9058 - accuracy: 0.8905 - val_loss: 2.6950 - val_accuracy: 0.7715\n",
      "Epoch 21/40\n",
      "40/40 [==============================] - 11s 270ms/step - loss: 0.8062 - accuracy: 0.9114 - val_loss: 2.6721 - val_accuracy: 0.7784\n",
      "Epoch 22/40\n",
      "40/40 [==============================] - 11s 272ms/step - loss: 0.7145 - accuracy: 0.9308 - val_loss: 2.6387 - val_accuracy: 0.7888\n",
      "Epoch 23/40\n",
      "40/40 [==============================] - 11s 267ms/step - loss: 0.6315 - accuracy: 0.9485 - val_loss: 2.6051 - val_accuracy: 0.7937\n",
      "Epoch 24/40\n",
      "40/40 [==============================] - 11s 271ms/step - loss: 0.5533 - accuracy: 0.9630 - val_loss: 2.5795 - val_accuracy: 0.7993\n",
      "Epoch 25/40\n",
      "40/40 [==============================] - 11s 273ms/step - loss: 0.4810 - accuracy: 0.9759 - val_loss: 2.5585 - val_accuracy: 0.8022\n",
      "Epoch 26/40\n",
      "40/40 [==============================] - 11s 272ms/step - loss: 0.4147 - accuracy: 0.9851 - val_loss: 2.5326 - val_accuracy: 0.8078\n",
      "Epoch 27/40\n",
      "40/40 [==============================] - 11s 268ms/step - loss: 0.3552 - accuracy: 0.9911 - val_loss: 2.5273 - val_accuracy: 0.8110\n",
      "Epoch 28/40\n",
      "40/40 [==============================] - 11s 267ms/step - loss: 0.3026 - accuracy: 0.9947 - val_loss: 2.5100 - val_accuracy: 0.8137\n",
      "Epoch 29/40\n",
      "40/40 [==============================] - 11s 279ms/step - loss: 0.2540 - accuracy: 0.9967 - val_loss: 2.4946 - val_accuracy: 0.8173\n",
      "Epoch 30/40\n",
      "40/40 [==============================] - 11s 273ms/step - loss: 0.2124 - accuracy: 0.9985 - val_loss: 2.4716 - val_accuracy: 0.8202\n",
      "Epoch 31/40\n",
      "40/40 [==============================] - 11s 274ms/step - loss: 0.1759 - accuracy: 0.9992 - val_loss: 2.4654 - val_accuracy: 0.8215\n",
      "Epoch 32/40\n",
      "40/40 [==============================] - 11s 270ms/step - loss: 0.1451 - accuracy: 0.9994 - val_loss: 2.4493 - val_accuracy: 0.8231\n",
      "Epoch 33/40\n",
      "40/40 [==============================] - 11s 276ms/step - loss: 0.1206 - accuracy: 0.9997 - val_loss: 2.4373 - val_accuracy: 0.8254\n",
      "Epoch 34/40\n",
      "40/40 [==============================] - 11s 271ms/step - loss: 0.1001 - accuracy: 0.9998 - val_loss: 2.4179 - val_accuracy: 0.8248\n",
      "Epoch 35/40\n",
      "40/40 [==============================] - 11s 270ms/step - loss: 0.0836 - accuracy: 0.9999 - val_loss: 2.4081 - val_accuracy: 0.8274\n",
      "Epoch 36/40\n",
      "40/40 [==============================] - 11s 273ms/step - loss: 0.0705 - accuracy: 0.9999 - val_loss: 2.3971 - val_accuracy: 0.8290\n",
      "Epoch 37/40\n",
      "40/40 [==============================] - 11s 271ms/step - loss: 0.0599 - accuracy: 0.9999 - val_loss: 2.3816 - val_accuracy: 0.8307\n",
      "Epoch 38/40\n",
      "40/40 [==============================] - 11s 269ms/step - loss: 0.0515 - accuracy: 1.0000 - val_loss: 2.3739 - val_accuracy: 0.8316\n",
      "Epoch 39/40\n",
      "40/40 [==============================] - 11s 267ms/step - loss: 0.0451 - accuracy: 0.9999 - val_loss: 2.3610 - val_accuracy: 0.8330\n",
      "Epoch 40/40\n",
      "40/40 [==============================] - 11s 274ms/step - loss: 0.0397 - accuracy: 0.9999 - val_loss: 2.3527 - val_accuracy: 0.8356\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x271db1eeb50>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_model, encoder_inputs, encoder_states = build_encoder()\n",
    "decoder_model_train, decoder_model_inf, decoder_inputs = build_decoder(encoder_states)\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_model_train([decoder_inputs] + encoder_states))\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.fit([eng_padded, hin_padded], hin_padded_cat, batch_size=64, epochs=40, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dda7cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krish\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save(\"eng_hin_translation.h5\")\n",
    "import pickle\n",
    "with open(\"eng_tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(eng_tokenizer, f)\n",
    "with open(\"hin_tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(hin_tokenizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5622ba26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 7s 79ms/step - loss: 0.2669 - accuracy: 0.9733\n",
      "Test Loss: 0.26686930656433105, Test Accuracy: 0.9733326435089111\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate([eng_padded, hin_padded], hin_padded_cat)\n",
    "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44d4d3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
